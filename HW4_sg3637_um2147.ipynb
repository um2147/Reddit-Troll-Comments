{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import scipy\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Data\n",
    "train = pd.read_csv('data/reddit_200k_train.csv', encoding = 'latin-1')\n",
    "train = train[['body','REMOVED']]\n",
    "\n",
    "test = pd.read_csv('data/reddit_200k_test.csv', encoding = 'latin-1')\n",
    "test = test[['body','REMOVED']]\n",
    "\n",
    "X_train = pd.DataFrame(train.body)\n",
    "y_train = pd.DataFrame(train.REMOVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167529 entries, 0 to 167528\n",
      "Data columns (total 1 columns):\n",
      "body    167529 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#Getting the info about the training set\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset it unbalanced, we would be undersampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129476, 1)\n",
      "(129476, 1)\n"
     ]
    }
   ],
   "source": [
    "#Undersampling the Data\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "X_train_subsample, y_train_subsample = rus.fit_sample(\n",
    "    X_train, y_train)\n",
    "print(X_train_subsample.shape)\n",
    "print(y_train_subsample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a baseline model\n",
    "count = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "base_pipeline = make_pipeline(count,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation Score of the baseline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {'auc': 'roc_auc', 'accuracy':'accuracy', 'average_precision': 'average_precision', \n",
    "           'precision':'precision', 'recall':'recall'}\n",
    "\n",
    "cv_baseline = cross_validate(base_pipeline, X_train_subsample.ravel(), y_train_subsample.ravel(), cv=3, scoring=scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy baseline: 0.686103968430159\n",
      "average_precision baseline: 0.7002991440316109\n",
      "precision baseline: 0.662622560552093\n",
      "recall baseline: 0.7583026878427024\n",
      "auc baseline: 0.7456980476638077\n"
     ]
    }
   ],
   "source": [
    "#Getting the result of the baseline model\n",
    "print(\"accuracy baseline: {}\".format(np.mean(cv_baseline['test_accuracy'])))\n",
    "print(\"average_precision baseline: {}\".format(np.mean(cv_baseline['test_average_precision'])))\n",
    "print(\"precision baseline: {}\".format(np.mean(cv_baseline['test_precision'])))\n",
    "print(\"recall baseline: {}\".format(np.mean(cv_baseline['test_recall'])))\n",
    "print(\"auc baseline: {}\".format(np.mean(cv_baseline['test_auc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 Other Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will improve our baseline model. Here we will implement three steps:\n",
    "\n",
    "- Remove the stop words (english stop words)\n",
    "- Remove the the words that are in less than 100 documents\n",
    "- Enforce an l2 penalty for the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stop words and Infrequent words\n",
    "count_1 = CountVectorizer(stop_words='english', min_df=100)\n",
    "base_pipeline_update = make_pipeline(count_1,LogisticRegression(penalty='l2'))\n",
    "\n",
    "cv_baseline_update = cross_validate(base_pipeline_update, X_train_subsample.ravel(), y_train_subsample.ravel(), cv=5, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy updated_baseline: 0.6814389945663553\n",
      "average_precision updated_baseline: 0.693983356147583\n",
      "precision updated_baseline: 0.6471149396013883\n",
      "recall updated_baseline: 0.7981247231679718\n",
      "auc updated_baseline: 0.738510409921234\n"
     ]
    }
   ],
   "source": [
    "#Getting the result of the updated baseline model with stop words and infrequent words removed\n",
    "print(\"accuracy updated_baseline: {}\".format(np.mean(cv_baseline_update['test_accuracy'])))\n",
    "print(\"average_precision updated_baseline: {}\".format(np.mean(cv_baseline_update['test_average_precision'])))\n",
    "print(\"precision updated_baseline: {}\".format(np.mean(cv_baseline_update['test_precision'])))\n",
    "print(\"recall updated_baseline: {}\".format(np.mean(cv_baseline_update['test_recall'])))\n",
    "print(\"auc updated_baseline: {}\".format(np.mean(cv_baseline_update['test_auc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the above that all the metric except the recall (which increases by about 4%) show a minor decrease in the performance. Next we will experiment with some more techniques to fine-tune the BoW model. The following steps would be used below:\n",
    "\n",
    "- Tfidf Vectorizer would be used with english stop words removed.\n",
    "- Grid Search over the C parameter for logistic regression model.\n",
    "\n",
    "CV=3 has been taken due to computational limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Creating Pipeline\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "pipeline_tfidf = Pipeline (steps = [('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                                    ('lr_tfidf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy log_tfidf: 0.6913018545144155\n",
      "average_precision log_tfidf: 0.7276490057083356\n",
      "precision log_tfidf: 0.6751870425459776\n",
      "recall log_tfidf: 0.7372949425667178\n",
      "auc log_tfidf: 0.7597363230621014\n"
     ]
    }
   ],
   "source": [
    "#Performing GridSearch with Tfidf\n",
    "parameters = {'lr_tfidf__C': np.logspace(0,4,10)}\n",
    "\n",
    "log_tfidf = GridSearchCV(pipeline_tfidf, param_grid = parameters, cv = 3, return_train_score = True, \n",
    "                         scoring=scoring,  refit='auc',n_jobs=-1)\n",
    "log_tfidf.fit(X_train_subsample.ravel(), y_train_subsample.ravel())\n",
    "\n",
    "#Cross-Validating the best model\n",
    "cv_log_tfidf = cross_validate(log_tfidf.best_estimator_, X_train_subsample.ravel(), \n",
    "                                    y_train_subsample.ravel(), cv=3, scoring=scoring)\n",
    "\n",
    "#Getting the results of the above model\n",
    "print(\"accuracy log_tfidf: {}\".format(np.mean(cv_log_tfidf['test_accuracy'])))\n",
    "print(\"average_precision log_tfidf: {}\".format(np.mean(cv_log_tfidf['test_average_precision'])))\n",
    "print(\"precision log_tfidf: {}\".format(np.mean(cv_log_tfidf['test_precision'])))\n",
    "print(\"recall log_tfidf: {}\".format(np.mean(cv_log_tfidf['test_recall'])))\n",
    "print(\"auc log_tfidf: {}\".format(np.mean(cv_log_tfidf['test_auc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the tfidf vectorizer does not improve the performance of the model, it remains almost the same. Now we will use a character n-gram model and a more comprehensive grid search. We will be tuning the following parameters in the model:\n",
    "\n",
    "- The C parameter of the Logistic Regression\n",
    "- N-grams: To take into account the neighboring words\n",
    "- Normalizer - Turned on/off\n",
    "\n",
    "The model will be run with CountVectorizer configuration with a char analyzer and English stop words removed. Note that a few parameters are tested due to the long model training time and limitations in the computation power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy grid_final: 0.7049105725750858\n",
      "average_precision grid_final: 0.763202981325995\n",
      "precision grid_final: 0.6978890131400032\n",
      "recall grid_final: 0.722651319218849\n",
      "auc grid_final: 0.7820082290275384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#More Comprehensive Grid-Search\n",
    "\n",
    "param_grid_final = {\"logisticregression__C\": [10, 1, 0.1],\n",
    "              \"countvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 3)],\n",
    "              \"normalizer\": [None, Normalizer()]\n",
    "             }\n",
    "\n",
    "grid_final = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\", stop_words='english'), Normalizer(), LogisticRegression(penalty='l2'),\n",
    "                                  memory=\"cache_folder\"),\n",
    "                    param_grid=param_grid_final, cv=3, scoring=scoring, refit='auc',n_jobs=-1\n",
    "                   )\n",
    "\n",
    "grid_final.fit(X_train_subsample.ravel(), y_train_subsample.ravel())\n",
    "\n",
    "cv_grid_final = cross_validate(grid_final.best_estimator_, X_train_subsample.ravel(), \n",
    "                                    y_train_subsample.ravel(), cv=3, scoring=scoring)\n",
    "\n",
    "#Getting the results of the above model\n",
    "print(\"accuracy grid_final: {}\".format(np.mean(cv_grid_final['test_accuracy'])))\n",
    "print(\"average_precision grid_final: {}\".format(np.mean(cv_grid_final['test_average_precision'])))\n",
    "print(\"precision grid_final: {}\".format(np.mean(cv_grid_final['test_precision'])))\n",
    "print(\"recall grid_final: {}\".format(np.mean(cv_grid_final['test_recall'])))\n",
    "print(\"auc grid_final: {}\".format(np.mean(cv_grid_final['test_auc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_final best parameters: {'countvectorizer__ngram_range': (2, 3), 'logisticregression__C': 10, 'normalizer': Normalizer(copy=True, norm='l2')}\n"
     ]
    }
   ],
   "source": [
    "print(\"grid_final best parameters: {}\".format(grid_final.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the scores have imporoved. Average precision and AUC increase the most (almost 5%). Now, we will create some new features and see if we can further enhance the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create new features and see if that affects the performance of our model. New features that will be created are:\n",
    "\n",
    "- Length of the String (number of words in the string)\n",
    "- Number of exclamation marks, full stops and commas\n",
    "- Number of capitalization in the text - i.e the number of words that are all capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>exc_count</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>length</th>\n",
       "      <th>capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You don't say. So making ppl work 12-24 hrs ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm actually looking for a PCP for the first t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>609</td>\n",
       "      <td>107</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Study: [Reconciling controversies about the â...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>944</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Journal reference:\\r\\n\\r\\nThe tumbling rotatio...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1766</td>\n",
       "      <td>241</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why can't we have fun, responsibly?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  comma_count  exc_count  \\\n",
       "0  You don't say. So making ppl work 12-24 hrs ha...            0          0   \n",
       "1  I'm actually looking for a PCP for the first t...            1          0   \n",
       "2  Study: [Reconciling controversies about the â...            7          0   \n",
       "3  Journal reference:\\r\\n\\r\\nThe tumbling rotatio...           25          0   \n",
       "4               Why can't we have fun, responsibly?             1          0   \n",
       "\n",
       "   dot_count  length  capital  \n",
       "0         87      16        0  \n",
       "1        609     107        8  \n",
       "2        944     122        1  \n",
       "3       1766     241        6  \n",
       "4         36       6        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe\n",
    "X_train_subsample_new = pd.DataFrame(X_train_subsample)\n",
    "X_train_subsample_new.columns = ['body']\n",
    "\n",
    "#Adding Features - Number of punctuations\n",
    "X_train_subsample_new['comma_count'] = X_train_subsample_new.body.str.count(',')\n",
    "X_train_subsample_new['exc_count'] = X_train_subsample_new.body.str.count('!')\n",
    "X_train_subsample_new['dot_count'] = X_train_subsample_new.body.str.count('.')\n",
    "\n",
    "#Adding Features - Number of words\n",
    "X_train_subsample_new['split_body'] = X_train_subsample_new.body.str.split()\n",
    "X_train_subsample_new['length'] = X_train_subsample_new.split_body.apply(lambda x: len(x))\n",
    "\n",
    "#Number of Capitalizations in the string\n",
    "X_train_subsample_new['capital'] = X_train_subsample_new.split_body.apply(lambda x: sum(map(str.isupper, x)))\n",
    "\n",
    "X_train_subsample_new.drop(columns=['split_body'], inplace=True)\n",
    "X_train_subsample_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Creating a function to vectorize the text and append the numberical features\n",
    "def vectorizer(df):\n",
    "    cvdf = CountVectorizer(stop_words='english').fit_transform(df.body)\n",
    "    return scipy.sparse.hstack((cvdf, df.drop(columns='body')))\n",
    "\n",
    "#Creating a transformed sample with the vectorizer\n",
    "X_train_subsample_transformed = FunctionTransformer(func=vectorizer, \n",
    "                                                    validate=False).fit_transform(X_train_subsample_new)\n",
    "\n",
    "\n",
    "cv_features = cross_validate(make_pipeline(LogisticRegression(penalty='l2')), X_train_subsample_transformed, y_train_subsample,\n",
    "                             cv=3, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy cv_features: 0.6829373721201702\n",
      "average_precision cv_features: 0.7004825478575661\n",
      "precision cv_features: 0.6567325518653209\n",
      "recall cv_features: 0.7666440886797358\n"
     ]
    }
   ],
   "source": [
    "#Getting the result of the updated baseline model with stop words and infrequent words removed\n",
    "print(\"accuracy cv_features: {}\".format(np.mean(cv_features['test_accuracy'])))\n",
    "print(\"average_precision cv_features: {}\".format(np.mean(cv_features['test_average_precision'])))\n",
    "print(\"precision cv_features: {}\".format(np.mean(cv_features['test_precision'])))\n",
    "print(\"recall cv_features: {}\".format(np.mean(cv_features['test_recall'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run a more comprehensive grid-search tuning the 'C' parameter in the logistic regression and with normalizer turned on and off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy grid_final: 0.689625851289676\n",
      "average_precision grid_final: 0.7114884932106532\n",
      "precision grid_final: 0.6553983153512535\n",
      "recall grid_final: 0.7997775679604064\n",
      "auc grid_final: 0.7523790685245612\n"
     ]
    }
   ],
   "source": [
    "#More Comprehensive Grid-Search\n",
    "param_grid_final = {\"logisticregression__C\": [10, 1, 0.1], \"normalizer\": [Normalizer(), None]}\n",
    "\n",
    "grid_feat = GridSearchCV(make_pipeline(Normalizer(), LogisticRegression(penalty='l2')),\n",
    "            param_grid=param_grid_final, cv=3, scoring=scoring, refit='auc',n_jobs=-1)\n",
    "\n",
    "grid_feat.fit(X_train_subsample_transformed, y_train_subsample)\n",
    "\n",
    "cv_grid_feat = cross_validate(grid_feat.best_estimator_, X_train_subsample_transformed, \n",
    "                                    y_train_subsample.ravel(), cv=3, scoring=scoring)\n",
    "\n",
    "#Getting the results of the above model\n",
    "print(\"accuracy grid_final: {}\".format(np.mean(cv_grid_feat['test_accuracy'])))\n",
    "print(\"average_precision grid_final: {}\".format(np.mean(cv_grid_feat['test_average_precision'])))\n",
    "print(\"precision grid_final: {}\".format(np.mean(cv_grid_feat['test_precision'])))\n",
    "print(\"recall grid_final: {}\".format(np.mean(cv_grid_feat['test_recall'])))\n",
    "print(\"auc grid_final: {}\".format(np.mean(cv_grid_feat['test_auc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_final best parameters: {'logisticregression__C': 0.1, 'normalizer': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"grid_final best parameters: {}\".format(grid_feat.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the above that the Grid Search without the normalizer performs better than the grid search with the normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the BOW analysis, we conclude that the logistic regression model with parameters - {'countvectorizer__ngram_range': (2, 3), 'logisticregression__C': 10, 'normalizer': Normalizer(copy=True, norm='l2')} and without added features performs the best. However, note that there is not much significant different between this model and the baseline model.\n",
    "\n",
    "All the other models that we tested in the analysis (tfidf, n-grams etc.) though they performance worse than this model, the difference is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations: We were only able to search over a few hyperparameters due to the computational limitation and the long running times of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, instead of taking BOW model, we will use some pretrained word embedding to evaluate if the performance of our model improves.\n",
    "\n",
    "We will use pretrained word vectors from FastText: https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "#Loading the pre-trained word vector from FastText\n",
    "from gensim.models.wrappers import FastText\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create a separate validation set (similar to the example in Prof. Mueller's notes), train the data on the training set and perform the validation on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting a validation set from the original training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train_sub, text_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_subsample, y_train_subsample, stratify=y_train_subsample, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the CountVectorizer\n",
    "vect_w2v = CountVectorizer(vocabulary=model.index2word)\n",
    "docs = vect_w2v.inverse_transform(vect_w2v.transform(text_train_sub.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all documents are vectorized due by the w2v embeddings, we will create a function that removes documents that have a length of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to remove the docs that are not vectorized using word2vec\n",
    "def removeBlankDocs(docInput, yInput):\n",
    "    i = 0\n",
    "    blankDocs = []\n",
    "    for doc in docInput:\n",
    "        if len(doc) == 0:\n",
    "            blankDocs.append(i)\n",
    "        i = i + 1\n",
    "    return(np.delete(docInput, blankDocs, axis=0), np.delete(yInput, blankDocs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the revised data using the above function\n",
    "X_train_w2v = removeBlankDocs(docs, y_train_sub)[0]\n",
    "y_train_w2v = removeBlankDocs(docs, y_train_sub)[1]\n",
    "\n",
    "X_train_final = np.vstack([np.mean(model[doc], axis=0) for doc in X_train_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating and fitting the model\n",
    "lr_w2v = LogisticRegression().fit(X_train_final, y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the necessary preprocessing on the validation set\n",
    "docs_val = vect_w2v.inverse_transform(vect_w2v.transform(text_val.ravel()))\n",
    "X_val_w2v = removeBlankDocs(docs_val, y_val)[0]\n",
    "y_val_w2v = removeBlankDocs(docs_val, y_val)[1]\n",
    "X_val_final = np.vstack([np.mean(model[doc], axis=0) for doc in X_val_w2v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us calculate the performance of the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6693353941267388\n",
      "AUC: 0.6693371302898056\n",
      "Average Precision: 0.6128822327216277\n",
      "Recall: 0.6749536178107607\n"
     ]
    }
   ],
   "source": [
    "#Calculating the validation scores\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Accuracy: \" + str(lr_w2v.score(X_val_final, y_val_w2v)))\n",
    "print(\"AUC: \" + str(roc_auc_score(y_val_w2v, lr_w2v.predict(X_val_final))))\n",
    "print(\"Average Precision: \" + str(average_precision_score(y_val_w2v, lr_w2v.predict(X_val_final))))\n",
    "print(\"Recall: \" + str(recall_score(y_val_w2v, lr_w2v.predict(X_val_final))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the word2vec model does not improve the performance compared to the best model that was obtained in the previous part. Hence, BOW model is prefered for the this dataset with our current implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coms007",
   "language": "python",
   "name": "coms007"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
